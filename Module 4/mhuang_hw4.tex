\documentclass{article}
\linespread{1.3}
\usepackage[margin=50pt]{geometry}
\usepackage{amsmath, amsthm, amssymb, amsthm, tikz, fancyhdr, graphicx, systeme}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\newcommand{\changefont}{\fontsize{15}{15}\selectfont}

\fancypagestyle{firstpageheader}{
  \fancyhead[R]{
    \changefont
    \parbox[t]{4cm}{ % Adjust width as needed
      Michael Huang\\
      EN.625.633.81\\
      Homework 4
    }
  }
}

\begin{document}

\thispagestyle{firstpageheader}
{\Large 

\section*{1.}

\subsection*{(a)} 

To find \(P(X_1 = 2)\), we can use the property that \(\pi^{(t)} = \pi^{(0)}P^t\), and do the simple weighted probability calculation accordingly:
\[
  P(X_1 = 2) = \sum_{i=0}^{2} P(X_0 = i) \cdot P(X_1 = 2 | X_0 = i)
\]
\[
  = \pi_0p_{02} + \pi_{1}p_{12} + \pi_2p_{22}
\]
\[
  = 0.2 \cdot 0.6 + 0.5 \cdot 0.2 + 0.3 \cdot 0.2
\]
\[
  = 0.12 + 0.10 + 0.06
\]
\[
  = \boxed{0.28}
\]

\subsection*{(b)}

To find \(P(X_2 = 2)\), we use the same property as before. We therefore need to calculate \(P^t = P^2\) in this case to get started:
\[
  P^2 = \begin{pmatrix} 0.3 & 0.1 & 0.6 \\ 0.4 & 0.4 & 0.2 \\ 0.1 & 0.7 & 0.2 \end{pmatrix} \begin{pmatrix} 0.3 & 0.1 & 0.6 \\ 0.4 & 0.4 & 0.2 \\ 0.1 & 0.7 & 0.2 \end{pmatrix}
\]
\[
  P^2 = 
  \begin{pmatrix} 
    (.3)(.3) + (.1)(.4) + (.6)(.1) &
    (.3)(.1) + (.1)(.4) + (.6)(.7) &
    (.3)(.6) + (.1)(.2) + (.6)(.2) \\
    (.4)(.3) + (.4)(.4) + (.2)(.1) &
    (.4)(.1) + (.4)(.4) + (.2)(.7) & 
    (.4)(.6) + (.4)(.2) + (.2)(.2) \\
    (.1)(.3) + (.7)(.4) + (.2)(.1) &
    (.1)(.1) + (.7)(.4) + (.2)(.7) &
    (.1)(.6) + (.7)(.2) + (.2)(.2)
  \end{pmatrix}
\]
\[
  P^2 = 
  \begin{pmatrix} 
    0.09 + 0.04 + 0.06 &
    0.03 + 0.04 + 0.42 &
    0.18 + 0.02 + 0.12 \\
    0.12 + 0.16 + 0.02 &
    0.04 + 0.16 + 0.14 &
    0.24 + 0.08 + 0.04 \\
    0.03 + 0.28 + 0.02 &
    0.01 + 0.28 + 0.14 &
    0.06 + 0.14 + 0.04
  \end{pmatrix}
\]
\[
  P^2 = 
  \begin{pmatrix} 
    0.19 & 0.49 & 0.32 \\ 
    0.30 & 0.34 & 0.36 \\ 
    0.33 & 0.43 & 0.24
  \end{pmatrix}
\] 
We can now plug in again:
\[
  P(X_2 = 2) = \pi^{(0)} \cdot \begin{pmatrix} 
    0.32 \\ 
    0.36 \\ 
    0.24
  \end{pmatrix}
\]
\[
  = 0.2 \cdot 0.32 + 0.5 \cdot 0.36 + 0.3 \cdot 0.24
\]
\[
  = 0.064 + 0.18 + 0.072
\]
\[
  = \boxed{0.316}
\]

\subsection*{(c)}

To find \(P(X_3 = 2 | X_0 = 0)\), we can use theorem 1 to take this as the \((0, 2)^{th}\) element in \(P^3\). So we first need to find \(P^3\):
\[
  P^3 = P^2 \cdot P
\]
\[
  P^3 = 
  \begin{pmatrix} 
    0.19 & 0.49 & 0.32 \\ 
    0.30 & 0.34 & 0.36 \\ 
    0.33 & 0.43 & 0.24
  \end{pmatrix} 
  \begin{pmatrix} 0.3 & 0.1 & 0.6 \\ 0.4 & 0.4 & 0.2 \\ 0.1 & 0.7 & 0.2 \end{pmatrix}
\]
\[
  P^3 = 
  \begin{pmatrix} 
    .19 \cdot .3 + .49 \cdot .4 + .32 \cdot .1 &
    .19 \cdot .1 + .49 \cdot .4 + .32 \cdot .7 &
    .19 \cdot .6 + .49 \cdot .2 + .32 \cdot .2 \\
    .30 \cdot .3 + .34 \cdot .4 + .36 \cdot .1 &
    .30 \cdot .1 + .34 \cdot .4 + .36 \cdot .7 & 
    .30 \cdot .6 + .34 \cdot .2 + .36 \cdot .2 \\
    .33 \cdot .3 + .43 \cdot .4 + .24 \cdot .1 &
    .33 \cdot .1 + .43 \cdot .4 + .24 \cdot .7 &
    .33 \cdot .6 + .43 \cdot .2 + .24 \cdot .2
  \end{pmatrix}
\]
\[
  P^3 = 
  \begin{pmatrix} 
    0.057 + 0.196 + 0.032 &
    0.019 + 0.196 + 0.224 &
    0.114 + 0.098 + 0.064 \\
    0.090 + 0.136 + 0.036 &
    0.030 + 0.136 + 0.252 &
    0.180 + 0.068 + 0.072 \\
    0.099 + 0.172 + 0.024 &
    0.033 + 0.172 + 0.168 &
    0.198 + 0.086 + 0.048
  \end{pmatrix}
\]
\[
  P^3 = 
  \begin{pmatrix} 
    0.285 & 0.439 & 0.276 \\ 
    0.262 & 0.418 & 0.320 \\ 
    0.295 & 0.373 & 0.332
  \end{pmatrix}
\]
Taking the corresponding element yields that \(\boxed{P(X_3 = 2 | X_0 = 0) = 0.276}\).

\subsection*{(d)}

To find \(P(X_0 = 1 | X_1 = 2)\), we can first decompose this expression into parts that we can calculate more easily using Bayes' theorem:
\[
  P(X_0 = 1 | X_1 = 2) = \frac{P(X_1 = 2 | X_0 = 1) \cdot P(X_0 = 1)}{P(X_1 = 2)}
\]
We can first substitute using \(P\), directly taking the \((1,2)^{th}\) element:
\[
  = \frac{0.2 \cdot P(X_0 = 1)}{P(X_1 = 2)}
\]
Next, we can substitute using \(\pi^{(0)}\):
\[
  = \frac{0.2 \cdot 0.5}{P(X_1 = 2)}
\]
Finally, we can substitute using what we found in part (a):
\[
  = \frac{0.10}{0.28} = \frac{10}{28} = \boxed{\frac{5}{14}}
\]

\subsection*{(e)}

To find \(P(X_1 = 1, X_3 = 1)\), we can first break this down by calculating the probabilities of the full chain, taking all states into account. In other words, we can set up the expression such that 
\[
  P(X_1 = 1, X_3 = 1) = \sum_{i=0}^{2} P(X_0 = i) \cdot P(X_1 = 1 | X_0 = i) \cdot P(X_3 = 1 | X_1 = 1)
\]
\[
  = P(X_3 = 1 | X_1 = 1) \sum_{i=0}^{2} \pi_i \cdot P_{i,1}
\]
Given the properties of the Markov chain, we can just treat this as a normal 2-step transition and therefore use \(P^2\) for this. So we can equate this to \(P^2_{1,1} \cdot P(X_1 = 1) \). Plug this in and evaluate these terms, given what we've already previously calculated: 
\[
  = P^2_{1,1} \cdot P(X_1 = 1)
\]
\[
  = 0.34 \cdot (\pi_0 \cdot P_{0,1} + \pi_1 \cdot P_{1,1} + \pi_2 \cdot P_{2,1})
\]
\[
  = 0.34 \cdot (0.2 \cdot 0.1 + 0.5 \cdot 0.4 + 0.3 \cdot 0.7)
\]
\[
  = 0.34 \cdot (0.02 + 0.2 + 0.21) 
\]
\[
  = 0.34 \cdot 0.43
\]
\[
  = \boxed{0.1462}
\]

}

{\Large 
\section*{2.}



}

{\Large 
\section*{3.}

\subsection*{(a)} 



\subsection*{(b)}



}

{\Large 
\section*{4.}

We essentially aim to show that 
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \{P(X_n = 1) - \frac{b}{a+b}\}
\]
As directed by the problem, we start with expanding \(P(X_{n+1} = 1)\) based on the condition of the chain at \(n\):
\[
  P(X_{n+1} = 1) = P(X_{n+1} = 1 | X_n = 1) \cdot P(X_n = 1) + P(X_{n+1} = 1 | X_n = 2) \cdot P(X_n = 2)
\]
And we now plug in using the given \(P\):
\[
  = (1 - a) \cdot P(X_n = 1) + b \cdot P(X_n = 2)
\]
We can also plug in \(P(X_n = 2) = 1 - P(X_n = 1)\):
\[
  = (1 - a) \cdot P(X_n = 1) + b \cdot (1 - P(X_n = 1))
\]
\[
  = (1 - a) \cdot P(X_n = 1) + b - b \cdot P(X_n = 1)
\]
\[
  = (1 - a - b) \cdot P(X_n = 1) + b
\]
Subtract \(\frac{b}{a+b}\) from both sides:
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot P(X_n = 1) + b - \frac{b}{a+b}
\]
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot P(X_n = 1) + \frac{b(a+b) - b}{a+b}
\]
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot P(X_n = 1) + \frac{ab + b^2 - b}{a+b}
\]
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot P(X_n = 1) + \frac{ab + b^2 - b}{a+b}
\]
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot P(X_n = 1) - \frac{(1 - a - b) \cdot b}{a+b}
\]
\[
  P(X_{n+1} = 1) - \frac{b}{a+b} = (1 - a - b) \cdot \{P(X_n = 1) - \frac{b}{a+b}\}
\]
as we expected to find in the first step. Let's continue by trying to find something to fit the exponent of the \((1 - a - b)\) term. We can more generally look at the terms as multiplying by a common factor of \((1 - a - b)\) by substituting the \(P(X_{n+1} = 1) - \frac{b}{a + b}\) value. In other words, by replacing with some arbitrary \(x_n = P(X_n = 1) - \frac{b}{a+b}\), we can simplify the expression to be 
\[
  x_{n+1} = (1 - a - b) \cdot x_n
\]
which yields the appropriate exponential form. In other words, we multiply by \((1 - a - b)\) each time we get the next term, yielding \((1 - a - b)^n\) term relative to \(x_0\), i.e. 
\[
  x_{n+1} = (1 - a - b)^{n+1} \cdot x_0
\]
or equivalently 
\[
  x_n = (1 - a - b)^n \cdot x_0
\]
and substituting back:
\[
  P(X_n = 1) - \frac{b}{a+b} = (1 - a - b)^n \cdot \{ P(X_0 = 1) - \frac{b}{a+b} \}
\]
\[
  P(X_n = 1) = \frac{b}{a+b} + (1 - a - b)^n \cdot \{ P(X_0 = 1) - \frac{b}{a+b} \}
\]
as we exactly sought to find. Evaluating the last statement, given \(0 \leq a + b \leq 2\), we can find that \(1 - (a + b)\) has range \([-1, 1]\), and given the exponential of \(n\) approaching infinity, the second term reaches 0, so \(P(X_n = 1)\) does indeed converge to be just the first term of \(\frac{b}{a+b}\).

}

\end{document}